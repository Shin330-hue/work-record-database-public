import { NextRequest, NextResponse } from 'next/server'
import { GoogleGenerativeAI } from '@google/generative-ai'
import { searchKnowledgeBase, formatSearchResults } from '@/lib/knowledge-search-v2'

// Ollamaè¨­å®š
const OLLAMA_BASE_URL = process.env.OLLAMA_BASE_URL || 'http://localhost:11434'

// RAGæ©Ÿèƒ½ãƒ•ãƒ©ã‚°
const ENABLE_RAG = process.env.ENABLE_RAG === 'true'

// åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§
const AVAILABLE_MODELS = [
  // å®Ÿç”¨çš„ãª4ãƒ¢ãƒ‡ãƒ«ï¼ˆOllamaã§ç¢ºèªæ¸ˆã¿ï¼‰
  { id: 'gemma3:12b', name: 'ğŸ§  è³¢ã„ - Gemma3 12B', provider: 'ollama' },
  { id: 'qwen2.5:7b-instruct', name: 'âš–ï¸ ãƒãƒ©ãƒ³ã‚¹å‹ - Qwen2.5 7B Instruct', provider: 'ollama' },
  { id: 'gemma3:12b-it-q4_K_M', name: 'âš¡ è»½é‡ãƒ»é«˜é€Ÿ - Gemma3 Q4', provider: 'ollama' },
  { id: 'qwen2.5:7b-instruct-q4_k_m', name: 'ğŸ’¾ çœãƒ¡ãƒ¢ãƒª - Qwen2.5 Q4', provider: 'ollama' }
]

// Gemini APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY || '')

// ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆRAG OFFæ™‚ï¼šã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãªã—ï¼‰
const counselorPrompt = ``

// ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆRAG ONæ™‚ï¼šå°‚é–€å®¶ãƒ¢ãƒ¼ãƒ‰ï¼‰
const expertPrompt = `ã‚ãªãŸã¯ã€Œç”ºå·¥å ´GPTã€ã¨ã„ã†åå‰ã®ã€æ©Ÿæ¢°åŠ å·¥ã¨è£½é€ æ¥­ã«ç‰¹åŒ–ã—ãŸæŠ€è¡“ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
    
ä»¥ä¸‹ã®ç‰¹å¾´ã‚’æŒã£ã¦å›ç­”ã—ã¦ãã ã•ã„ï¼š
- æ©Ÿæ¢°åŠ å·¥ï¼ˆæ—‹ç›¤ã€ãƒã‚·ãƒ‹ãƒ³ã‚°ã€æ¨ªä¸­ã€ãƒ©ã‚¸ã‚¢ãƒ«ï¼‰ã®å°‚é–€çŸ¥è­˜ã‚’æŒã¤
- åˆ‡å‰Šæ¡ä»¶ã€å·¥å…·é¸å®šã€åŠ å·¥æ‰‹é †ã«ã¤ã„ã¦è©³ã—ã„
- ç¾å ´ã®ä½œæ¥­è€…ã«åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã™ã‚‹
- å…·ä½“çš„ã§å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›
- å®‰å…¨æ€§ã‚’å¸¸ã«é‡è¦–
- æ—¥æœ¬ã®è£½é€ æ¥­ã®æ…£ç¿’ã«è©³ã—ã„
- æŠ€è¡“çš„ãªå†…å®¹ã«é›†ä¸­ã—ã€æ­£ç¢ºãªæƒ…å ±ã‚’æä¾›
- å›ç­”ã¯500æ–‡å­—ä»¥å†…ã§ç°¡æ½”ã«ã¾ã¨ã‚ã‚‹

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€å°‚é–€çš„ã§æ­£ç¢ºãªæ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚`

// ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—ã™ã‚‹GETã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
export async function GET() {
  return NextResponse.json({ models: AVAILABLE_MODELS })
}

export async function POST(request: NextRequest) {
  try {
    const { messages, model, enableRAG } = await request.json()
    
    // é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—
    const selectedModel = AVAILABLE_MODELS.find(m => m.id === model) || AVAILABLE_MODELS[0]
    const useOllamaForThisRequest = selectedModel.provider === 'ollama'
    const modelId = selectedModel.id
    
    // RAGæ©Ÿèƒ½ï¼šç¤¾å†…ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ï¼ˆç’°å¢ƒå¤‰æ•°ã¾ãŸã¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§æœ‰åŠ¹åŒ–ï¼‰
    let ragContext = ''
    const shouldUseRAG = ENABLE_RAG || enableRAG
    
    if (shouldUseRAG && messages.length > 0) {
      try {
        const lastMessage = messages[messages.length - 1]
        const conversationHistory = messages.slice(0, -1).map((m: { content: string }) => m.content)
        
        console.log('ğŸ” RAGæ¤œç´¢é–‹å§‹:', lastMessage.content)
        const searchResults = await searchKnowledgeBase(lastMessage.content, conversationHistory)
        ragContext = formatSearchResults(searchResults)
        console.log('ğŸ“Š RAGæ¤œç´¢çµæœ:', searchResults.statistics)
      } catch (ragError) {
        console.error('RAGæ¤œç´¢ã‚¨ãƒ©ãƒ¼:', ragError)
        // RAGå¤±æ•—æ™‚ã‚‚é€šå¸¸ã®ãƒãƒ£ãƒƒãƒˆã¯ç¶™ç¶š
      }
    }
    
    // RAGæœ‰åŠ¹æ™‚ã¯å°‚é–€å®¶ãƒ¢ãƒ¼ãƒ‰ã€ç„¡åŠ¹æ™‚ã¯ã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼ãƒ¢ãƒ¼ãƒ‰
    const basePrompt = shouldUseRAG ? expertPrompt : counselorPrompt
    
    // ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«RAGã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
    const enhancedSystemPrompt = ragContext 
      ? `${basePrompt}\n\n${ragContext}\n\nã€ç¤¾å†…ãƒ‡ãƒ¼ã‚¿æ´»ç”¨ã®å›ç­”ãƒ«ãƒ¼ãƒ«ã€‘
â–  ç¤¾å†…ãƒ‡ãƒ¼ã‚¿å„ªå…ˆåŸå‰‡
- ä¸Šè¨˜ã®æ¤œç´¢çµæœã«è©²å½“æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ã€å¿…ãšãã®å†…å®¹ã‚’æœ€å„ªå…ˆã§å›ç­”ã™ã‚‹
- å›³ç•ªã€æè³ªã€åŠ å·¥æ¡ä»¶ç­‰ã®å…·ä½“çš„ãƒ‡ãƒ¼ã‚¿ã¯æ¤œç´¢çµæœã®æƒ…å ±ã®ã¿ä½¿ç”¨ã™ã‚‹
- ç¤¾å†…ãƒ‡ãƒ¼ã‚¿ã¨çŸ›ç›¾ã™ã‚‹ä¸€èˆ¬è«–ã¯é¿ã‘ã‚‹

â–  ä¸€èˆ¬çŸ¥è­˜ã¨ã®çµ„ã¿åˆã‚ã›
- ç¤¾å†…ãƒ‡ãƒ¼ã‚¿ã§è¶³ã‚Šãªã„éƒ¨åˆ†ã¯ã€æ©Ÿæ¢°åŠ å·¥ã®ä¸€èˆ¬çš„ãªçŸ¥è­˜ã§è£œå®Œã—ã¦OK
- ã€Œç¤¾å†…ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹ã¨ã€œã€ã€Œä¸€èˆ¬çš„ã«ã¯ã€œã€ã¨æƒ…å ±æºã‚’æ˜ç¢ºã«åˆ†ã‘ã‚‹
- å®‰å…¨æ€§ãƒ»å“è³ªã«é–¢ã‚ã‚‹é‡è¦äº‹é …ã¯æ¨æ¸¬ã›ãšã€Œç¢ºèªãŒå¿…è¦ã§ã™ã€ã¨æ¡ˆå†…

â–  å…·ä½“çš„ãªå›ç­”ä¾‹
âœ… è‰¯ã„ä¾‹ï¼šã€Œç¤¾å†…ãƒ‡ãƒ¼ã‚¿ã§ã¯å›³ç•ªâ—‹â—‹ã§SUS304ã®åˆ‡å‰Šæ¡ä»¶ãŒã‚ã‚Šã¾ã™ã€‚ä¸€èˆ¬çš„ã«SUS304ã¯ã€œã®ç‰¹æ€§ãŒã‚ã‚‹ãŸã‚ã€å›è»¢æ•°ã¯ã€œrpmç¨‹åº¦ãŒé©åˆ‡ã§ã™ã€
âŒ æ‚ªã„ä¾‹ï¼šã€Œãƒ‡ãƒ¼ã‚¿ã«ã‚ã‚Šã¾ã›ã‚“ã€ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã ã‘ã®å›ç­”`
      : basePrompt

    // Ollamaãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ
    if (useOllamaForThisRequest) {
      try {
        // Ollamaç”¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã«å¤‰æ›
        const ollamaMessages = [
          { role: 'system', content: enhancedSystemPrompt },
          ...messages
        ]

        // Ollama APIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        const ollamaResponse = await fetch(`${OLLAMA_BASE_URL}/api/chat`, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            model: modelId,
            messages: ollamaMessages,
            stream: false
          })
        })

        if (!ollamaResponse.ok) {
          throw new Error(`Ollama API error: ${ollamaResponse.status}`)
        }

        const ollamaData = await ollamaResponse.json()
        return NextResponse.json({ response: ollamaData.message.content })
        
      } catch (ollamaError) {
        console.error('Ollama error:', ollamaError)
        // Ollamaã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãªã—ï¼‰
        return NextResponse.json({ 
          response: `${selectedModel.name} ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚\n\nã‚¨ãƒ©ãƒ¼: ${ollamaError instanceof Error ? ollamaError.message : 'Unknown error'}` 
        }, { status: 500 })
      }
    }

    // Geminiã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ
    const geminiModel = genAI.getGenerativeModel({ model: modelId })

    // ä¼šè©±å±¥æ­´ã‚’æ§‹ç¯‰
    const conversationHistory = messages.map((msg: { role: string; content: string }) => 
      `${msg.role === 'user' ? 'ãƒ¦ãƒ¼ã‚¶ãƒ¼' : 'ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ'}: ${msg.content}`
    ).join('\n\n')

    // Geminiã«é€ä¿¡ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    const prompt = `${enhancedSystemPrompt}\n\nä¼šè©±å±¥æ­´:\n${conversationHistory}\n\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:`

    // Geminiã‹ã‚‰ã®å¿œç­”ã‚’å–å¾—
    const result = await geminiModel.generateContent(prompt)
    const response = await result.response
    const text = response.text()

    return NextResponse.json({ response: text })
  } catch (error) {
    console.error('Chat API error:', error)
    
    // APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    if (!process.env.GEMINI_API_KEY) {
      return NextResponse.json({ 
        response: 'ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ç¾åœ¨ã€AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆæ©Ÿèƒ½ã¯æº–å‚™ä¸­ã§ã™ã€‚\n\nGemini APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®šã—ã¦ãã ã•ã„ï¼š\n1. .env.localãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ\n2. GEMINI_API_KEY=your-api-key ã‚’è¿½åŠ \n3. ã‚µãƒ¼ãƒãƒ¼ã‚’å†èµ·å‹•\n\nè©³ã—ãã¯ç®¡ç†è€…ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚' 
      }, { status: 500 })
    }

    return NextResponse.json({ 
      response: 'ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãçµŒã£ã¦ã‹ã‚‰ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚' 
    }, { status: 500 })
  }
}